{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEAM NAME: GEEK GURUS\n",
    "\n",
    "## Problem Statement: \n",
    "You are tasked with developing a voice-based profile unlock system - a smart voice\n",
    "lock that can accurately identify and open a person's profile based on their voice. The\n",
    "system will be used to grant secure access to individual user profiles in various\n",
    "applications, devices, or online platforms. Participants are expected to build a robust\n",
    "and reliable software solution that can distinguish between different users by\n",
    "analysing their unique voice characteristics.\n",
    "\n",
    "### REQUIREMENTS:\n",
    "● Voice Data Collection :- 18 precollected dataset and 7 individual dataset.\\\n",
    "● Voice Feature Extraction :- Short Time Fourier Transform(STFT) followed by converting amplitude to dB scale and finalling plotting spectogram of each sample.\\\n",
    "● Machine Learning Model:- Trained Convolutional Neural Network.\\\n",
    "● Simple user profile management\n",
    "\n",
    "## Results:\n",
    "### ● Trained a CNN model for Voice Recognition that is 92.58% accurate in predicting the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary modules like numpy, pandas, matplotlib, tensorflow, PIL, sklearn and pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=\"./train/images/\"\n",
    "folders=os.listdir(dataset_path)\n",
    "\n",
    "# Initializing training and test dataset\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "\n",
    "# Split the dataset into training and test set.\n",
    "num=np.random.rand(3960)\n",
    "mask=num<0.2\n",
    "split=mask.astype(int)\n",
    "\n",
    "i=0\n",
    "for dirs in folders:\n",
    "    for img in os.listdir(str(dataset_path+dirs)):\n",
    "        image=Image.open(str(dataset_path+dirs+'/'+img))\n",
    "        new_img=image.resize((200,200))\n",
    "        tmp_array=np.array(new_img)/255.\n",
    "        if split[i]==0:\n",
    "            X_train.append(tmp_array)\n",
    "            y_train.append(str(dirs))\n",
    "        else:\n",
    "            X_test.append(tmp_array)\n",
    "            y_test.append(str(dirs))\n",
    "        \n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding dependent variables using Label-Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aew': 0,\n",
       " 'ahw': 1,\n",
       " 'aup': 2,\n",
       " 'awb': 3,\n",
       " 'axb': 4,\n",
       " 'bdl': 5,\n",
       " 'Chaitanya': 6,\n",
       " 'clb': 7,\n",
       " 'eey': 8,\n",
       " 'fem': 9,\n",
       " 'gka': 10,\n",
       " 'Harsh': 11,\n",
       " 'Himanshu': 12,\n",
       " 'jmk': 13,\n",
       " 'Krupesh': 14,\n",
       " 'ksp': 15,\n",
       " 'ljm': 16,\n",
       " 'lnh': 17,\n",
       " 'Natvar': 18,\n",
       " 'rms': 19,\n",
       " 'rxr': 20,\n",
       " 'Shashank': 21,\n",
       " 'slp': 22,\n",
       " 'slt': 23,\n",
       " 'Takshay': 24}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict={}\n",
    "i=0\n",
    "for val in folders:\n",
    "    dict[val]=i\n",
    "    i=i+1\n",
    "\n",
    "dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labelling y_train and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for val in y_train:\n",
    "    y_train[i]=dict[y_train[i]]\n",
    "    i=i+1\n",
    "\n",
    "i=0\n",
    "for val in y_test:\n",
    "    y_test[i]=dict[y_test[i]]\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we were trying to differentiate people based on the spectogram images of their voice, we thought that convolutional Neural Network would be the best choice to deal with image dataset. The following model involves two convolutional layer and two max pulling as well as two ReLU layers.\n",
    "\n",
    "Convolutional layer helps increase the computational efficiency and max pulling layers helps us extract the dominant features very important to detect the frequency difference in the voice of two people.\n",
    "\n",
    "ReLU layer introduce Non-Linearity as demanded by some complex functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as tfl\n",
    "\n",
    "def convolutional_model(input_shape):\n",
    "    input_img = tf.keras.Input(shape=input_shape)\n",
    "    Z1=tfl.Conv2D(filters=8,kernel_size=(4,4),strides=(1,1),padding='same')(input_img)\n",
    "    A1=tfl.ReLU()(Z1)\n",
    "    P1=tfl.MaxPool2D(pool_size=(8,8),strides=(8,8),padding='same')(A1)\n",
    "    Z2=tfl.Conv2D(filters=16,kernel_size=(2,2),strides=(1,1),padding='same')(P1)\n",
    "    A2=tfl.ReLU()(Z2)\n",
    "    P2=tfl.MaxPool2D(pool_size=(4,4),strides=(4,4),padding='same')(A2)\n",
    "    F=tfl.Flatten()(P2)\n",
    "    outputs=tfl.Dense(25,activation='softmax')(F)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_img, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we were having the dependent variable label encoded, the clear choice of loss funciton was sparse categorical crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = convolutional_model((200, 200, 4))\n",
    "conv_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are implementing mini batch algorithm to train our model faster and we are using our test dataset as our cross validation dataset and training the model for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 26s 453ms/step - loss: 3.5100 - accuracy: 0.0000e+00 - val_loss: 3.2175 - val_accuracy: 0.0443\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 21s 428ms/step - loss: 3.2214 - accuracy: 0.0294 - val_loss: 3.2153 - val_accuracy: 0.0807\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 22s 436ms/step - loss: 3.2172 - accuracy: 0.0354 - val_loss: 3.1965 - val_accuracy: 0.0495\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 20s 402ms/step - loss: 3.1849 - accuracy: 0.0063 - val_loss: 3.1403 - val_accuracy: 0.0599\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 22s 441ms/step - loss: 3.1133 - accuracy: 0.0000e+00 - val_loss: 3.0414 - val_accuracy: 0.1003\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 22s 431ms/step - loss: 3.0219 - accuracy: 0.0254 - val_loss: 2.9489 - val_accuracy: 0.1432\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 21s 416ms/step - loss: 2.9152 - accuracy: 0.0746 - val_loss: 3.5115 - val_accuracy: 0.1328\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 20s 403ms/step - loss: 3.0820 - accuracy: 0.0833 - val_loss: 2.9406 - val_accuracy: 0.0833\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 21s 419ms/step - loss: 2.8874 - accuracy: 0.0614 - val_loss: 2.7868 - val_accuracy: 0.0977\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 22s 438ms/step - loss: 3.0010 - accuracy: 0.0016 - val_loss: 2.8586 - val_accuracy: 0.1302\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 22s 439ms/step - loss: 2.8617 - accuracy: 0.0473 - val_loss: 2.8004 - val_accuracy: 0.1302\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 20s 401ms/step - loss: 2.8105 - accuracy: 0.0836 - val_loss: 2.7474 - val_accuracy: 0.1393\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 20s 398ms/step - loss: 2.8310 - accuracy: 0.0489 - val_loss: 2.7929 - val_accuracy: 0.1562\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 21s 419ms/step - loss: 2.7742 - accuracy: 0.1093 - val_loss: 2.6882 - val_accuracy: 0.2031\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 20s 401ms/step - loss: 2.7679 - accuracy: 0.0652 - val_loss: 2.7834 - val_accuracy: 0.1875\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 20s 401ms/step - loss: 2.7451 - accuracy: 0.1460 - val_loss: 2.6674 - val_accuracy: 0.2057\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 20s 408ms/step - loss: 2.6999 - accuracy: 0.1786 - val_loss: 2.6414 - val_accuracy: 0.2279\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 21s 414ms/step - loss: 2.5468 - accuracy: 0.1880 - val_loss: 2.2714 - val_accuracy: 0.3008\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 20s 402ms/step - loss: 2.3108 - accuracy: 0.2538 - val_loss: 2.0632 - val_accuracy: 0.3620\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 20s 402ms/step - loss: 2.4784 - accuracy: 0.1773 - val_loss: 2.0686 - val_accuracy: 0.3945\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 20s 398ms/step - loss: 2.2074 - accuracy: 0.3123 - val_loss: 2.5392 - val_accuracy: 0.2812\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 20s 399ms/step - loss: 2.3375 - accuracy: 0.2641 - val_loss: 2.0114 - val_accuracy: 0.3867\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 22s 438ms/step - loss: 1.9661 - accuracy: 0.2904 - val_loss: 1.6992 - val_accuracy: 0.4258\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 20s 406ms/step - loss: 1.8506 - accuracy: 0.3127 - val_loss: 1.4124 - val_accuracy: 0.4909\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 20s 399ms/step - loss: 1.7782 - accuracy: 0.3033 - val_loss: 1.3108 - val_accuracy: 0.5977\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 20s 396ms/step - loss: 1.6130 - accuracy: 0.3712 - val_loss: 1.3227 - val_accuracy: 0.6016\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 1.5205 - accuracy: 0.4355 - val_loss: 1.2958 - val_accuracy: 0.5312\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 20s 398ms/step - loss: 1.5073 - accuracy: 0.4292 - val_loss: 1.1399 - val_accuracy: 0.5846\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 20s 403ms/step - loss: 1.4544 - accuracy: 0.4048 - val_loss: 1.0140 - val_accuracy: 0.6406\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 20s 399ms/step - loss: 1.3807 - accuracy: 0.4333 - val_loss: 0.9441 - val_accuracy: 0.6797\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 20s 403ms/step - loss: 1.3024 - accuracy: 0.4684 - val_loss: 0.8953 - val_accuracy: 0.6979\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 1.2431 - accuracy: 0.4875 - val_loss: 0.8495 - val_accuracy: 0.7292\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 21s 414ms/step - loss: 1.1888 - accuracy: 0.5119 - val_loss: 0.8091 - val_accuracy: 0.7435\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 20s 398ms/step - loss: 1.1331 - accuracy: 0.5354 - val_loss: 0.7739 - val_accuracy: 0.7474\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 20s 398ms/step - loss: 1.0824 - accuracy: 0.5595 - val_loss: 0.7406 - val_accuracy: 0.7630\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 20s 398ms/step - loss: 1.0307 - accuracy: 0.5840 - val_loss: 0.7073 - val_accuracy: 0.7760\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 20s 398ms/step - loss: 0.9806 - accuracy: 0.6175 - val_loss: 0.6777 - val_accuracy: 0.7852\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 20s 397ms/step - loss: 0.9326 - accuracy: 0.6369 - val_loss: 0.6487 - val_accuracy: 0.7930\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 20s 398ms/step - loss: 0.8897 - accuracy: 0.6648 - val_loss: 0.6227 - val_accuracy: 0.8060\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 25s 499ms/step - loss: 0.8496 - accuracy: 0.6845 - val_loss: 0.5947 - val_accuracy: 0.8151\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 20s 406ms/step - loss: 0.8129 - accuracy: 0.7002 - val_loss: 0.5688 - val_accuracy: 0.8242\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 23s 469ms/step - loss: 0.7790 - accuracy: 0.7105 - val_loss: 0.5454 - val_accuracy: 0.8294\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 22s 448ms/step - loss: 0.7491 - accuracy: 0.7231 - val_loss: 0.5220 - val_accuracy: 0.8333\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 21s 418ms/step - loss: 0.7204 - accuracy: 0.7296 - val_loss: 0.4994 - val_accuracy: 0.8438\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 20s 402ms/step - loss: 0.6950 - accuracy: 0.7425 - val_loss: 0.4790 - val_accuracy: 0.8424\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 21s 427ms/step - loss: 0.6728 - accuracy: 0.7534 - val_loss: 0.4606 - val_accuracy: 0.8451\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 24s 474ms/step - loss: 0.6531 - accuracy: 0.7591 - val_loss: 0.4458 - val_accuracy: 0.8594\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.6344 - accuracy: 0.7644 - val_loss: 0.4402 - val_accuracy: 0.8555\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 24s 475ms/step - loss: 0.6184 - accuracy: 0.7669 - val_loss: 0.4439 - val_accuracy: 0.8542\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 20s 404ms/step - loss: 0.6035 - accuracy: 0.7697 - val_loss: 0.4558 - val_accuracy: 0.8568\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 21s 418ms/step - loss: 0.5860 - accuracy: 0.7788 - val_loss: 0.4875 - val_accuracy: 0.8490\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 23s 467ms/step - loss: 0.5744 - accuracy: 0.7870 - val_loss: 0.5185 - val_accuracy: 0.8438\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 22s 436ms/step - loss: 0.5671 - accuracy: 0.7939 - val_loss: 0.5319 - val_accuracy: 0.8333\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 24s 478ms/step - loss: 0.5538 - accuracy: 0.8064 - val_loss: 0.5118 - val_accuracy: 0.8424\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 27s 530ms/step - loss: 0.5302 - accuracy: 0.8133 - val_loss: 0.4825 - val_accuracy: 0.8490\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 26s 511ms/step - loss: 0.5040 - accuracy: 0.8192 - val_loss: 0.4463 - val_accuracy: 0.8581\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 0.4766 - accuracy: 0.8280 - val_loss: 0.4071 - val_accuracy: 0.8659\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 20s 399ms/step - loss: 0.4491 - accuracy: 0.8459 - val_loss: 0.3710 - val_accuracy: 0.8867\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 20s 396ms/step - loss: 0.4287 - accuracy: 0.8509 - val_loss: 0.3437 - val_accuracy: 0.8841\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 21s 413ms/step - loss: 0.4128 - accuracy: 0.8553 - val_loss: 0.3248 - val_accuracy: 0.8867\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 21s 417ms/step - loss: 0.3997 - accuracy: 0.8596 - val_loss: 0.3104 - val_accuracy: 0.8919\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 21s 413ms/step - loss: 0.3898 - accuracy: 0.8622 - val_loss: 0.3001 - val_accuracy: 0.8958\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 24s 471ms/step - loss: 0.3817 - accuracy: 0.8669 - val_loss: 0.2938 - val_accuracy: 0.9023\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 20s 400ms/step - loss: 0.3739 - accuracy: 0.8647 - val_loss: 0.2911 - val_accuracy: 0.8997\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 23s 453ms/step - loss: 0.3672 - accuracy: 0.8716 - val_loss: 0.2932 - val_accuracy: 0.8893\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 19s 382ms/step - loss: 0.3630 - accuracy: 0.8728 - val_loss: 0.3080 - val_accuracy: 0.8854\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 19s 378ms/step - loss: 0.3664 - accuracy: 0.8716 - val_loss: 0.3295 - val_accuracy: 0.8698\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 19s 375ms/step - loss: 0.3728 - accuracy: 0.8593 - val_loss: 0.3828 - val_accuracy: 0.8555\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 19s 378ms/step - loss: 0.3877 - accuracy: 0.8518 - val_loss: 0.4174 - val_accuracy: 0.8503\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 0.4124 - accuracy: 0.8365 - val_loss: 0.5478 - val_accuracy: 0.8229\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 23s 461ms/step - loss: 0.4534 - accuracy: 0.8217 - val_loss: 0.5872 - val_accuracy: 0.8047\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 21s 410ms/step - loss: 0.4752 - accuracy: 0.8174 - val_loss: 0.4954 - val_accuracy: 0.8385\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 20s 407ms/step - loss: 0.4344 - accuracy: 0.8183 - val_loss: 0.4763 - val_accuracy: 0.8646\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 21s 420ms/step - loss: 0.4468 - accuracy: 0.8289 - val_loss: 0.4157 - val_accuracy: 0.8958\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 20s 395ms/step - loss: 0.5218 - accuracy: 0.8430 - val_loss: 0.5112 - val_accuracy: 0.8620\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 19s 390ms/step - loss: 0.4961 - accuracy: 0.8631 - val_loss: 0.3505 - val_accuracy: 0.8919\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 19s 371ms/step - loss: 0.4248 - accuracy: 0.8684 - val_loss: 0.4337 - val_accuracy: 0.8633\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 18s 368ms/step - loss: 0.4021 - accuracy: 0.8810 - val_loss: 0.3165 - val_accuracy: 0.8906\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 18s 364ms/step - loss: 0.3703 - accuracy: 0.8775 - val_loss: 0.4089 - val_accuracy: 0.8672\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 18s 365ms/step - loss: 0.3762 - accuracy: 0.8853 - val_loss: 0.2888 - val_accuracy: 0.8984\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 20s 401ms/step - loss: 0.3390 - accuracy: 0.8816 - val_loss: 0.3752 - val_accuracy: 0.8763\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 19s 369ms/step - loss: 0.3452 - accuracy: 0.8907 - val_loss: 0.2684 - val_accuracy: 0.9049\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 20s 403ms/step - loss: 0.3107 - accuracy: 0.8916 - val_loss: 0.3467 - val_accuracy: 0.8893\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 21s 423ms/step - loss: 0.3194 - accuracy: 0.8982 - val_loss: 0.2550 - val_accuracy: 0.9076\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 22s 449ms/step - loss: 0.2893 - accuracy: 0.8994 - val_loss: 0.3247 - val_accuracy: 0.8958\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 19s 378ms/step - loss: 0.2989 - accuracy: 0.9044 - val_loss: 0.2454 - val_accuracy: 0.9102\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 19s 385ms/step - loss: 0.2767 - accuracy: 0.8985 - val_loss: 0.3116 - val_accuracy: 0.8971\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 23s 455ms/step - loss: 0.2861 - accuracy: 0.9076 - val_loss: 0.2385 - val_accuracy: 0.9102\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 25s 503ms/step - loss: 0.2650 - accuracy: 0.9057 - val_loss: 0.3005 - val_accuracy: 0.8971\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 22s 441ms/step - loss: 0.2742 - accuracy: 0.9082 - val_loss: 0.2329 - val_accuracy: 0.9141\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 21s 431ms/step - loss: 0.2553 - accuracy: 0.9126 - val_loss: 0.2921 - val_accuracy: 0.8971\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 21s 425ms/step - loss: 0.2646 - accuracy: 0.9148 - val_loss: 0.2283 - val_accuracy: 0.9167\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 19s 370ms/step - loss: 0.2472 - accuracy: 0.9145 - val_loss: 0.2865 - val_accuracy: 0.8997\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 19s 370ms/step - loss: 0.2568 - accuracy: 0.9170 - val_loss: 0.2246 - val_accuracy: 0.9206\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 18s 369ms/step - loss: 0.2409 - accuracy: 0.9204 - val_loss: 0.2838 - val_accuracy: 0.8997\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 18s 368ms/step - loss: 0.2515 - accuracy: 0.9179 - val_loss: 0.2221 - val_accuracy: 0.9219\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 18s 369ms/step - loss: 0.2355 - accuracy: 0.9214 - val_loss: 0.2822 - val_accuracy: 0.9023\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 19s 370ms/step - loss: 0.2470 - accuracy: 0.9201 - val_loss: 0.2195 - val_accuracy: 0.9232\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 18s 368ms/step - loss: 0.2306 - accuracy: 0.9217 - val_loss: 0.2810 - val_accuracy: 0.9023\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 18s 369ms/step - loss: 0.2430 - accuracy: 0.9223 - val_loss: 0.2170 - val_accuracy: 0.9258\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(64)\n",
    "history = conv_model.fit(train_dataset, epochs=100, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations: \n",
    "We observed that the model's training accuracy is almost similar to cross validation set accuracy which shows that our model does not overfit and would generalize well on any unseen Audio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are preprocessing the new input sample before feeding it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=Image.open(str(r\"C:\\Users\\Shashank\\Documents\\Tic-Tech-Toe-2023\\temp\\images\\Himanshu\\output_audio_0.png\"))\n",
    "new_img=image.resize((200,200))\n",
    "NewImageTaken=np.array(new_img)/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of training and running the model everytime for predicting new audio, we are creating pickle file which stores the model architecture and weights to be used for the future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'conv_model.sav'\n",
    "pickle.dump(conv_model, open(filename, 'wb'))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pickle file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename, 'rb') as file:\n",
    "    load_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are expanding the dimensions of image to make it compatible to model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 91ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predicted = load_model.predict(tf.expand_dims(NewImageTaken,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1872256e-06, 4.1412035e-20, 6.3200451e-07, 1.3269940e-07,\n",
       "        5.3057758e-14, 4.5136471e-08, 8.5392458e-05, 5.9769088e-19,\n",
       "        1.8380447e-08, 1.5651175e-11, 2.0856106e-13, 5.3420980e-03,\n",
       "        9.5312518e-01, 7.2516121e-21, 4.1396145e-02, 2.5276115e-09,\n",
       "        2.2915698e-09, 2.4920964e-05, 1.7026417e-05, 6.4641296e-20,\n",
       "        2.9011366e-11, 7.2174117e-08, 4.2623812e-15, 1.7539805e-09,\n",
       "        7.1127365e-06]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each element of the predicted vector would represent the likelihood of that labelled being the input audio. We would be selecting that has maximum likelihood and map it to the corresponding dictionary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum element is 0.9531251788139343 at index (0, 12)\n"
     ]
    }
   ],
   "source": [
    "max_element = float('-inf')\n",
    "max_element_index = None\n",
    "\n",
    "for i, sublist in enumerate(y_predicted):\n",
    "    for j, element in enumerate(sublist):\n",
    "        if element > max_element:\n",
    "            max_element = element\n",
    "            max_element_index = (i, j)\n",
    "\n",
    "print(f\"The maximum element is {max_element} at index {max_element_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Himanshu\n"
     ]
    }
   ],
   "source": [
    "for key, value in dict.items():\n",
    "    if value == max_element_index[1]:\n",
    "        result_key = key\n",
    "        break  \n",
    "\n",
    "print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
